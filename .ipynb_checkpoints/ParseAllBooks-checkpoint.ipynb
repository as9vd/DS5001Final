{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2a3a68-7017-415d-b36d-105ffd6d5fcd",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "edda4e51-1fbd-44e2-8d4d-b70b63a45f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073ac82-0925-44c7-94ed-acc34d0881b1",
   "metadata": {},
   "source": [
    "## Generate Tables for Each Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c248e4b0-fb99-4a30-bdd5-d4806b89ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "source_file_list = sorted(glob(\"Books/Text Files/**\"))\n",
    "LIB = pd.read_csv('Busby Example Notebooks/CSV Dataframes/LIB.csv').set_index(\"book_id\")\n",
    "\n",
    "LIB.iloc[3][\"chap_regex\"] = \"nan\"\n",
    "LIB.iloc[4][\"chap_regex\"] = '^(1[0-7]|[1-9])\\.|(Prologue|Epilogue)$' # Charlton's first\n",
    "LIB.iloc[5][\"chap_regex\"] = '^(?:[1-9]|1[0-9]|2[0-9]|PROLOGUE|EPILOGUE)$' # Charlton's second\n",
    "LIB.iloc[6][\"chap_regex\"] = '^\\d{1,2}\\.\\d{1,2}\\.\\d{2}$' # Ferguson's first\n",
    "LIB.iloc[8][\"chap_regex\"] = '^(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|twenty[- ]one|twenty[- ]two|twenty[- ]three|twenty[- ]four|twenty[- ]five)' # Ferguson's third\n",
    "LIB.iloc[10][\"chap_regex\"] = '^(1[0-7]|[1-9])\\\\.|(INTRODUCTION)$' # Robbo\n",
    "LIB.iloc[11][\"chap_regex\"] = '(?<!\\\\d)(?:1[0-8]|[1-9])(?!\\\\d)' # Scholesy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e70b044b-85a5-4432-8499-1ed4dbd39c54",
   "metadata": {},
   "source": [
    "file = \"Books/Text Files/Robson.txt\"\n",
    "\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "df = pd.DataFrame({'text': lines})\n",
    "df = df.replace('\\n', '', regex = True) # just formatting it a bit bud\n",
    "df['text'] = df['text'].str.strip()\n",
    "df = df[~df.apply(lambda row: row.str.contains('^\\s*$', regex=True)).all(axis=1)]\n",
    "df = df.reset_index()\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "regex = \n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    chapter_rows = [index for index, text in df.iloc[:, 0].iteritems() if re.match(regex, text)]\n",
    "            \n",
    "print(file)\n",
    "if i != 10:\n",
    "    for row_num in chapter_rows:\n",
    "        print(df.iloc[row_num].text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "09bc2206-e3de-49da-8a1c-7dedc997c526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books/Text Files/Atkinson1.txt',\n",
       " 'Books/Text Files/Atkinson2.txt',\n",
       " 'Books/Text Files/Busby1.txt',\n",
       " 'Books/Text Files/Busby2.txt',\n",
       " 'Books/Text Files/Charlton1.txt',\n",
       " 'Books/Text Files/Charlton2.txt',\n",
       " 'Books/Text Files/Ferguson1.txt',\n",
       " 'Books/Text Files/Ferguson2.txt',\n",
       " 'Books/Text Files/Ferguson3.txt',\n",
       " 'Books/Text Files/Keane.txt',\n",
       " 'Books/Text Files/Robson.txt',\n",
       " 'Books/Text Files/Scholes.txt']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB[\"source_file_path\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5cf6d1f1-6cec-4703-a5f4-a142b63fd302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "\n",
    "iteration_count = 0\n",
    "# Eamon Dunphy's booked is fucked.\n",
    "import warnings\n",
    "for file in source_file_list:\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # These lines below simply break up the text into a Dataframe with an unnamed index equivalent to the line number.\n",
    "    # The other column is the text contained in that line.\n",
    "    # It's just a bunch of formatting here.\n",
    "    df = pd.DataFrame({'text': lines})\n",
    "    df = df.replace('\\n', '', regex = True) # just formatting it a bit bud\n",
    "    df['text'] = df['text'].str.strip()\n",
    "    df = df[~df.apply(lambda row: row.str.contains('^\\s*$', regex=True)).all(axis=1)]\n",
    "    df = df.reset_index()\n",
    "    df = df.iloc[: , 1:]\n",
    "    \n",
    "    # Get the regex I manually compiled and use it to split up the text into chapters.\n",
    "    regex = LIB[LIB[\"source_file_path\"] == file][\"chap_regex\"].get(key = iteration_count)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        chapter_rows = [index for index, text in df.iloc[:, 0].iteritems() if re.match(regex, text)]\n",
    "                \n",
    "    if file != \"Books/Text Files/Busby2.txt\": # Skip Dunphy's book. Fantastic book but basically illegible .txt.\n",
    "        i = 0\n",
    "        for row_num in chapter_rows:\n",
    "            row_dicts = dict()\n",
    "            for row_num in chapter_rows: # row_dicts is a dictionary that stores chapter number to row.\n",
    "                row_dicts[i] = row_num\n",
    "                i += 1\n",
    "            \n",
    "        chapter_contents = {}\n",
    "        current_chapter = None\n",
    "        current_chapter_start = None\n",
    "        for x, row in df.iterrows():\n",
    "            # check if this row starts a new chapter\n",
    "            if x - 1 in row_dicts.values():\n",
    "                # if this row starts a new chapter, update the current chapter and its start line\n",
    "                current_chapter = list(row_dicts.keys())[list(row_dicts.values()).index(x - 1)]\n",
    "                current_chapter_start = x - 1\n",
    "                chapter_contents[current_chapter] = \"\"\n",
    "            # if we're in the middle of a chapter, add the row contents to the current chapter's contents\n",
    "            if current_chapter is not None:\n",
    "                chapter_contents[current_chapter] += row['text'] + \" \"\n",
    "                \n",
    "        CHAPS = pd.Series(chapter_contents).to_frame()\n",
    "        CHAPS = CHAPS.reset_index().rename(columns = {\"index\": \"chap_num\"}).set_index(\"chap_num\")\n",
    "        CHAPS.rename(columns = {0: \"chap_str\"}, inplace = 1)\n",
    "        dfs[file] = CHAPS\n",
    "        \n",
    "    iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3107516d-9d30-422c-93f1-5add2b12b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE\n",
      "TWO\n",
      "THREE\n",
      "FOUR\n",
      "FIVE\n",
      "SIX\n",
      "SEVEN\n",
      "EIGHT\n",
      "NINE\n",
      "TEN\n",
      "ELEVEN\n",
      "TWELVE\n",
      "THIRTEEN\n",
      "FOURTEEN\n",
      "FIFTEEN\n",
      "SIXTEEN\n",
      "SEVENTEEN\n",
      "EIGHTEEN\n",
      "NINETEEN\n",
      "TWENTY\n",
      "TWENTY-ONE\n"
     ]
    }
   ],
   "source": [
    "for num in chapter_rows:\n",
    "    print(df.iloc[num].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d553c-f224-4feb-9688-4c124bc3eb68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
