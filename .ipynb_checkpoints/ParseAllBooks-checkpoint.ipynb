{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2a3a68-7017-415d-b36d-105ffd6d5fcd",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "edda4e51-1fbd-44e2-8d4d-b70b63a45f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c8ffe-0ea3-4250-8c0d-7e285e8b66cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073ac82-0925-44c7-94ed-acc34d0881b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Tables for Each Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9fb87-f6e8-437d-9af5-d8615f4ff50f",
   "metadata": {},
   "source": [
    "Unfortunately, due to the imparsibility of Mr. Dunphy's book, I'm omitting it from the analysis. Fantastic read, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c248e4b0-fb99-4a30-bdd5-d4806b89ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "source_file_list = sorted(glob(\"Books/Text Files/**\"))\n",
    "LIB = pd.read_csv('Busby Example Notebooks/CSV Dataframes/LIB.csv').set_index(\"book_id\")\n",
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "LIB.iloc[3][\"chap_regex\"] = \"nan\"\n",
    "LIB.iloc[4][\"chap_regex\"] = '^(1[0-7]|[1-9])\\.|(Prologue|Epilogue)$' # Charlton's first\n",
    "LIB.iloc[5][\"chap_regex\"] = '^(?:[1-9]|1[0-9]|2[0-9]|PROLOGUE|EPILOGUE)$' # Charlton's second\n",
    "LIB.iloc[6][\"chap_regex\"] = '^\\d{1,2}\\.\\d{1,2}\\.\\d{2}$' # Ferguson's first\n",
    "LIB.iloc[8][\"chap_regex\"] = '^(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|twenty[- ]one|twenty[- ]two|twenty[- ]three|twenty[- ]four|twenty[- ]five)' # Ferguson's third\n",
    "LIB.iloc[10][\"chap_regex\"] = '^(1[0-7]|[1-9])\\\\.|(INTRODUCTION)$' # Robbo\n",
    "LIB.iloc[11][\"chap_regex\"] = '(?<!\\\\d)(?:1[0-8]|[1-9])(?!\\\\d)' # Scholesy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4305de-4e9c-4cb6-a4a8-c9d112d888cd",
   "metadata": {},
   "source": [
    "#### CHAPS and PARAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8a439-792b-4161-a876-0a95e1491795",
   "metadata": {},
   "source": [
    "This incredibly long-winded piece of code effectively works in 6 parts:\n",
    "1. Turn the .txt file into a Dataframe.\n",
    "2. Filter that Dataframe to get rid of blank rows, \\n's, and excess whitespace.\n",
    "3. Use the predefined LIB regexes to find where chapters start (and end).\n",
    "4. Use these markers to find the contents of each chapter.\n",
    "5. Split the chapters into paragraphs by splitting via new lines and find the contents of each paragraph.\n",
    "6. Store these paragraph Dataframes into a dictionary for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5cf6d1f1-6cec-4703-a5f4-a142b63fd302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "\n",
    "iteration_count = 0\n",
    "# Eamon Dunphy's booked is fucked.\n",
    "import warnings\n",
    "for file in source_file_list:\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # These lines below simply break up the text into a Dataframe with an unnamed index equivalent to the line number.\n",
    "    # The other column is the text contained in that line.\n",
    "    # It's just a bunch of formatting here.\n",
    "    df = pd.DataFrame({'text': lines})\n",
    "    df = df.replace('\\n', '', regex = True) # just formatting it a bit bud\n",
    "    df['text'] = df['text'].str.strip()\n",
    "    df = df[~df.apply(lambda row: row.str.contains('^\\s*$', regex=True)).all(axis=1)]\n",
    "    df = df.reset_index()\n",
    "    df = df.iloc[: , 1:]\n",
    "    \n",
    "    # Get the regex I manually compiled and use it to split up the text into chapters.\n",
    "    regex = LIB[LIB[\"source_file_path\"] == file][\"chap_regex\"].get(key = iteration_count)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        # Get the rows matching the start of each chapter.\n",
    "        chapter_rows = [index for index, text in df.iloc[:, 0].iteritems() if re.match(regex, text)]\n",
    "                \n",
    "    if file != \"Books/Text Files/Busby2.txt\": # Skip Dunphy's book. Fantastic book but basically illegible .txt.\n",
    "        i = 1\n",
    "        row_dicts = dict()\n",
    "        for row_num in chapter_rows: # row_dicts is a dictionary that stores chapter number to row.\n",
    "            row_dicts[i] = row_num\n",
    "            i += 1\n",
    "            \n",
    "        chapter_contents = {}\n",
    "        current_chapter = None\n",
    "        current_chapter_start = None\n",
    "        for x, row in df.iterrows():\n",
    "            # Check if this row starts a new chapter\n",
    "            if x - 1 in row_dicts.values():\n",
    "                # If this row starts a new chapter, update the current chapter and its start line\n",
    "                current_chapter = list(row_dicts.keys())[list(row_dicts.values()).index(x - 1)]\n",
    "                current_chapter_start = x - 1\n",
    "                chapter_contents[current_chapter] = \"\"\n",
    "            # If we're in the middle of a chapter, add the row contents to the current chapter's contents\n",
    "            if current_chapter is not None:\n",
    "                chapter_contents[current_chapter] += row['text'] + \" \"\n",
    "             \n",
    "        # Turning the dict into a DataFrame.\n",
    "        CHAPS = pd.Series(chapter_contents).to_frame()\n",
    "        CHAPS = CHAPS.reset_index().rename(columns = {\"index\": \"chap_num\"}).set_index(\"chap_num\")\n",
    "        CHAPS.rename(columns = {0: \"chap_str\"}, inplace = 1)\n",
    "        \n",
    "        para_pat = r'\\n\\n+'\n",
    "\n",
    "        # Split the chapters based on whitespace into its constitituent paragraphs.\n",
    "        PARAS = CHAPS['chap_str'].str.split(para_pat, expand=True).stack()\\\n",
    "                      .to_frame('para_str').sort_index()\n",
    "        PARAS.index.names = OHCO[:2]\n",
    "        PARAS['para_str'] = PARAS['para_str'].str.strip() # Update the index names and fix the formatting a bit.\n",
    "        \n",
    "        # Do the same thing as above.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            chapter_paragraphs = {}\n",
    "            current_chapter = None\n",
    "            current_chapter_start = None\n",
    "            paragraph_number = 0\n",
    "            df_paragraphs = pd.DataFrame(columns=['chapter', 'paragraph', 'text'])\n",
    "            i = 0\n",
    "            for i, row in df.iterrows():\n",
    "                # check if this row starts a new chapter\n",
    "                if i - 1 in row_dicts.values():\n",
    "                    # if this row starts a new chapter, update the current chapter and its start line\n",
    "                    current_chapter = list(row_dicts.keys())[list(row_dicts.values()).index(i - 1)]\n",
    "                    current_chapter_start = i - 1\n",
    "                    chapter_paragraphs[current_chapter] = \"\"\n",
    "                    paragraph_number = 0\n",
    "                # if we're in the middle of a chapter, add the row contents to the current chapter's paragraphs\n",
    "                if current_chapter is not None:\n",
    "                    paragraph_number += 1\n",
    "                    paragraph_text = row['text']\n",
    "                    chapter_paragraphs[current_chapter] += paragraph_text + \"\\n\"\n",
    "                    df_paragraphs = df_paragraphs.append({'chapter': current_chapter, 'paragraph': paragraph_number, \n",
    "                                                  'text': paragraph_text}, ignore_index = True)\n",
    "                    \n",
    "        PARAS = df_paragraphs\n",
    "        PARAS.rename(columns = {\"chapter\": \"chap_num\", \"paragraph\": \"para_num\", \"text\": \"para_str\"}, inplace = 1)\n",
    "        PARAS.set_index([\"chap_num\", \"para_num\"], inplace = True)\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x))).stack().to_frame('sent_str')\n",
    "        SENTS.index.names = OHCO[:3]\n",
    "        \n",
    "        dfs[file] = SENTS\n",
    "        \n",
    "    iteration_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b16452-ec26-440e-9f2d-f2e907c9271b",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "d68e7b77-2d38-4732-8382-3831f1d270b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books/Text Files/Atkinson1.txt',\n",
       " 'Books/Text Files/Atkinson2.txt',\n",
       " 'Books/Text Files/Busby1.txt',\n",
       " 'Books/Text Files/Busby2.txt',\n",
       " 'Books/Text Files/Charlton1.txt',\n",
       " 'Books/Text Files/Charlton2.txt',\n",
       " 'Books/Text Files/Ferguson1.txt',\n",
       " 'Books/Text Files/Ferguson2.txt',\n",
       " 'Books/Text Files/Ferguson3.txt',\n",
       " 'Books/Text Files/Keane.txt',\n",
       " 'Books/Text Files/Robson.txt',\n",
       " 'Books/Text Files/Scholes.txt']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB[\"source_file_path\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7b9d553c-f224-4feb-9688-4c124bc3eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sent_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>Press conferences are usually held at Carringt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If it is a Champions League week his briefings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>Today is the first time we have seen him this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United play Debrecen of Hungary in a Champions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everyone is happy to be back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">97</th>\n",
       "      <th>36</th>\n",
       "      <th>3</th>\n",
       "      <td>Of course he can be infuriating but he would s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">37</th>\n",
       "      <th>0</th>\n",
       "      <td>An educated guess is that Ferguson has two mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But it would be no surprise if he were still i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Or maybe Neville’s correct and Ferguson will s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life at Old Trafford might be an emotional rol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5651 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     sent_str\n",
       "chap_num para_num sent_num                                                   \n",
       "1        1        0         Press conferences are usually held at Carringt...\n",
       "                  1         If it is a Champions League week his briefings...\n",
       "         2        0         Today is the first time we have seen him this ...\n",
       "                  1         United play Debrecen of Hungary in a Champions...\n",
       "                  2                             Everyone is happy to be back.\n",
       "...                                                                       ...\n",
       "97       36       3         Of course he can be infuriating but he would s...\n",
       "         37       0         An educated guess is that Ferguson has two mor...\n",
       "                  1         But it would be no surprise if he were still i...\n",
       "                  2         Or maybe Neville’s correct and Ferguson will s...\n",
       "                  3         Life at Old Trafford might be an emotional rol...\n",
       "\n",
       "[5651 rows x 1 columns]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"Books/Text Files/Ferguson1.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "177f1245-51a5-467c-850d-cd6b3f684495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sent_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>In 1993, the 84th year of Sir Matt Busby’s lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the behest of a television company he was t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>United, now managed by Alex Ferguson with Busb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With Bobby Charlton, Pat Crerand, Alex Stepney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Collins fondly recalled Busby ‘puffing his pip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">15</th>\n",
       "      <th>97</th>\n",
       "      <th>0</th>\n",
       "      <td>The Busby Babes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>Nearly 60 years after Munich, with José Mourin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paul Scholes, one of United’s most revered pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But had clearly got to know the old man and un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For Scholes, one March evening in 2016, was wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7641 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     sent_str\n",
       "chap_num para_num sent_num                                                   \n",
       "1        1        0         In 1993, the 84th year of Sir Matt Busby’s lif...\n",
       "                  1         At the behest of a television company he was t...\n",
       "         2        0         United, now managed by Alex Ferguson with Busb...\n",
       "                  1         With Bobby Charlton, Pat Crerand, Alex Stepney...\n",
       "                  2         Collins fondly recalled Busby ‘puffing his pip...\n",
       "...                                                                       ...\n",
       "15       97       0                                           The Busby Babes\n",
       "         98       0         Nearly 60 years after Munich, with José Mourin...\n",
       "                  1         Paul Scholes, one of United’s most revered pla...\n",
       "                  2         But had clearly got to know the old man and un...\n",
       "                  3         For Scholes, one March evening in 2016, was wo...\n",
       "\n",
       "[7641 rows x 1 columns]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"Books/Text Files/Busby1.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8044bc7-0b8b-454a-97e9-16472d8399ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combining TOKENS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d43eecec-b5c0-4ccb-8c43-a500157900e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = LIB[\"source_file_path\"].to_list()\n",
    "for i in range(12): # This adds the book_id to the outer index.\n",
    "    if i == 3:\n",
    "        continue\n",
    "    df = dfs[paths[i]]\n",
    "    \n",
    "    new_levels = pd.Index([i] * len(df.index), name='book_id')\n",
    "    new_index = pd.MultiIndex.from_arrays([new_levels] + [df.index.get_level_values(level) \n",
    "                                                          for level in range(df.index.nlevels)], names=['book_id'] + df.index.names)\n",
    "    df.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "7e7dc905-62b8-4246-aae7-aa79b41e1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack these Dataframes on top of one another to get a CORPUS.\n",
    "combined = pd.DataFrame([]) # Combined sentences.\n",
    "for path in paths:\n",
    "    if path == \"Books/Text Files/Busby2.txt\": # Sorry Eamon Dunphy.\n",
    "        continue\n",
    "    combined = pd.concat([combined, dfs[path]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "e5452df6-33f7-4446-986e-56f7d56fa619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sent_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>THEY WERE ALIENS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOT THE ALIENS WE WOULD READ ABOUT in the comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They looked like gods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>It was December 1954; I was fifteen, a ground-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can still see Ferenc Puskas walking through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">18</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>But for all the benefits that professional foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We are happy where we live – on the edge of Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’d like to echo the words used by Sir Bobby C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He said, ‘I’ve been a lucky, lucky lad.’ And t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>THE HEART IS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57519 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             sent_str\n",
       "book_id chap_num para_num sent_num                                                   \n",
       "0       1        1        0                                         THEY WERE ALIENS.\n",
       "                          1         NOT THE ALIENS WE WOULD READ ABOUT in the comi...\n",
       "                          2                                    They looked like gods.\n",
       "                 2        0         It was December 1954; I was fifteen, a ground-...\n",
       "                          1         I can still see Ferenc Puskas walking through ...\n",
       "...                                                                               ...\n",
       "11      18       3        0         But for all the benefits that professional foo...\n",
       "                          1         We are happy where we live – on the edge of Sa...\n",
       "                          2         I’d like to echo the words used by Sir Bobby C...\n",
       "                          3         He said, ‘I’ve been a lucky, lucky lad.’ And t...\n",
       "                 4        0                                              THE HEART IS\n",
       "\n",
       "[57519 rows x 1 columns]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "e1d7582c-c671-48c3-a1df-201a3c44c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tokens = pd.DataFrame([])\n",
    "for path in paths:\n",
    "    if path == \"Books/Text Files/Busby2.txt\":\n",
    "        continue\n",
    "        \n",
    "    SENTS = dfs[path]\n",
    "        \n",
    "    # M04\n",
    "    # Turn the sentences in the current DFs dictionary into tokens, something I can actually use.\n",
    "    TOKENS = SENTS.sent_str.apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))).stack().to_frame('pos_tuple')\n",
    "    TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "    TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "    TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "\n",
    "    # https://www.techiedelight.com/remove-non-alphanumeric-characters-string-python/\n",
    "    import string\n",
    "    def remove_nonalphanumeric(text):\n",
    "        return ''.join(char for char in text if char.isalnum())\n",
    "\n",
    "    term_str = TOKENS[\"term_str\"].apply(remove_nonalphanumeric) # Remove non-alphanumeric characters in the Dataframe.\n",
    "    TOKENS[\"term_str\"] = term_str\n",
    "    \n",
    "    combined_tokens = pd.concat([combined_tokens, TOKENS])\n",
    "    \n",
    "combined_tokens.index.set_names(['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9498f1bd-726a-4e2c-bfbb-6d2bc6f6ff2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(THEY, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>THEY</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(WERE, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>WERE</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(ALIENS., NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ALIENS.</td>\n",
       "      <td>aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>(NOT, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NOT</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(THE, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>THE</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">18</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>14</th>\n",
       "      <td>(me,, NNS)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>me,</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(too., NNS)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>too.</td>\n",
       "      <td>too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(THE, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>THE</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(HEART, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>HEART</td>\n",
       "      <td>heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(IS, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>IS</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050784 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   pos_tuple  pos token_str  \\\n",
       "book_id chap_num para_num sent_num token_num                                  \n",
       "0       1        1        0        0             (THEY, NNP)  NNP      THEY   \n",
       "                                   1             (WERE, NNP)  NNP      WERE   \n",
       "                                   2          (ALIENS., NNP)  NNP   ALIENS.   \n",
       "                          1        0              (NOT, NNP)  NNP       NOT   \n",
       "                                   1              (THE, NNP)  NNP       THE   \n",
       "...                                                      ...  ...       ...   \n",
       "11      18       3        3        14             (me,, NNS)  NNS       me,   \n",
       "                                   15            (too., NNS)  NNS      too.   \n",
       "                 4        0        0               (THE, DT)   DT       THE   \n",
       "                                   1            (HEART, NNP)  NNP     HEART   \n",
       "                                   2               (IS, NNP)  NNP        IS   \n",
       "\n",
       "                                             term_str  \n",
       "book_id chap_num para_num sent_num token_num           \n",
       "0       1        1        0        0             they  \n",
       "                                   1             were  \n",
       "                                   2           aliens  \n",
       "                          1        0              not  \n",
       "                                   1              the  \n",
       "...                                               ...  \n",
       "11      18       3        3        14              me  \n",
       "                                   15             too  \n",
       "                 4        0        0              the  \n",
       "                                   1            heart  \n",
       "                                   2               is  \n",
       "\n",
       "[1050784 rows x 4 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b25bfd15-10cb-455e-b761-f380c291a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tokens.to_csv('TOKENS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc9710-6939-4890-96ff-ad2429b0c05f",
   "metadata": {},
   "source": [
    "## Combining VOCAB Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2b44da1e-00b0-4401-aec4-c92234bbf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "3d1180d3-4c95-4eed-b25d-a70fa872075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vocab = pd.DataFrame([])\n",
    "for i in range(12):\n",
    "    if i == 3:\n",
    "        continue\n",
    "        \n",
    "    TOKENS = combined_tokens.loc[i]\n",
    "    VOCAB = TOKENS.term_str.value_counts().to_frame('n')\n",
    "    VOCAB.index.name = 'term_str'\n",
    "    VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "    VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "    VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "    \n",
    "    # https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    VOCAB['max_pos'] = TOKENS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "    VOCAB = VOCAB[VOCAB[\"n_chars\"] > 0] # get rid of just the blank whitespace\n",
    "    \n",
    "    sw = pd.DataFrame({'stop': 1}, index=nltk.corpus.stopwords.words('english')) # marking stopwords mate\n",
    "    sw.index.name='term_str'\n",
    "    if 'stop' not in VOCAB.columns:\n",
    "        VOCAB = VOCAB.join(sw)\n",
    "        VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n",
    "        \n",
    "    # m04\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer1 = PorterStemmer()\n",
    "    VOCAB['stem_porter'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer2 = SnowballStemmer(\"english\")\n",
    "    VOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n",
    "\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    stemmer3 = LancasterStemmer()\n",
    "    VOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)\n",
    "    \n",
    "    if 'term_rank' not in VOCAB.columns: # just adding a new column and ranking them\n",
    "        VOCAB = VOCAB.sort_values('n', ascending=False).reset_index()\n",
    "        VOCAB.index.name = 'term_rank' \n",
    "        VOCAB = VOCAB.reset_index()\n",
    "        VOCAB = VOCAB.set_index('term_str')\n",
    "        VOCAB['term_rank'] = VOCAB['term_rank'] + 1\n",
    "    \n",
    "    new_rank = VOCAB.n.value_counts()\\\n",
    "                .sort_index(ascending=False).reset_index().reset_index()\\\n",
    "                .rename(columns={'level_0':'term_rank2', 'index':'n', 'n':'nn'})\\\n",
    "                .set_index('n')\n",
    "    VOCAB['term_rank2'] = VOCAB.n.map(new_rank.term_rank2) + 1\n",
    "    \n",
    "    VOCAB['zipf_k'] = VOCAB.n * VOCAB.term_rank\n",
    "    VOCAB['zipf_k2'] = VOCAB.p * VOCAB.term_rank2\n",
    "    \n",
    "    new_levels = pd.Index([i] * len(VOCAB.index), name = 'book_id')\n",
    "    new_index = pd.MultiIndex.from_arrays([new_levels] + [VOCAB.index.get_level_values(level) \n",
    "                                                          for level in range(VOCAB.index.nlevels)], names=['book_id'] + VOCAB.index.names)\n",
    "    VOCAB.index = new_index\n",
    "    \n",
    "    combined_vocab = pd.concat([combined_vocab, VOCAB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557beda-786a-4a36-8a9a-474b388c3410",
   "metadata": {},
   "source": [
    "## BOW and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "3714feb8-dfa5-485e-8adc-b5c5bf54a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAPS = OHCO[:2]\n",
    "bag = CHAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "568beabd-af92-41ca-b47b-36d18c5beb29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_BOW = pd.DataFrame([])\n",
    "for i in range(12):\n",
    "    if i == 3:\n",
    "        continue\n",
    "        \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        TOKENS = combined_tokens.loc[i]\n",
    "        BOW = TOKENS.groupby([\"chap_num\"]+['term_str']).term_str.count().to_frame('n') \n",
    "        BOW = BOW.drop(BOW[BOW.index.get_level_values('term_str') == ''].index) # drop the whitespace mate\n",
    "    \n",
    "        N = DTCM.shape[0]\n",
    "    \n",
    "        DTCM = BOW.n.unstack().fillna(0).astype('int')\n",
    "    \n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "        TF = TF.T\n",
    "    \n",
    "        DF = DTCM.astype('bool').sum()\n",
    "        IDF = np.log2(N / DF)\n",
    "    \n",
    "        TFIDF = TF * IDF\n",
    "    \n",
    "        DOC = DTCM.sum(1).to_frame('n_tokens')\n",
    "        DOC['n_types'] = DTCM.astype('bool').sum(1)\n",
    "    \n",
    "        VOCAB = combined_vocab.iloc[i]\n",
    "        VOCAB['df'] = DF\n",
    "        VOCAB['idf'] = IDF\n",
    "        VOCAB['tfidf_mean'] = TFIDF.mean() \n",
    "        VOCAB['tfidf_sum'] = TFIDF.sum()\n",
    "        VOCAB['tfidf_median'] = TFIDF.median()\n",
    "        VOCAB['tfidf_max'] = TFIDF.max()\n",
    "\n",
    "        VOCAB['dfidf'] = VOCAB.df * VOCAB.idf\n",
    "        VOCAB['dp'] = VOCAB.df / len(DOC)\n",
    "        VOCAB['dh'] = VOCAB.dp * np.log2(1/VOCAB.dp)\n",
    "        \n",
    "        BOW['tf'] = TF.stack()\n",
    "        BOW['tfidf'] = TFIDF.stack()\n",
    "    \n",
    "        new_levels = pd.Index([i] * len(BOW.index), name = 'book_id')\n",
    "        new_index = pd.MultiIndex.from_arrays([new_levels] + [BOW.index.get_level_values(level) \n",
    "                                                          for level in range(BOW.index.nlevels)], names=['book_id'] + BOW.index.names)\n",
    "        BOW.index = new_index\n",
    "        \n",
    "        combined_BOW = pd.concat([combined_BOW, BOW])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "f306af9d-a663-4f73-87c5-7d9ddf0a53a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1721yearolds</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950s</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.004551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">18</th>\n",
       "      <th>wife</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.004390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>-0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>2</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>-0.000789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287131 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               n        tf     tfidf\n",
       "book_id chap_num term_str                           \n",
       "0       1        1721yearolds  1  0.000364  0.001517\n",
       "                 1950s         1  0.000364  0.001153\n",
       "                 1953          3  0.001091  0.004551\n",
       "                 1954          1  0.000364  0.001153\n",
       "                 1956          1  0.000364  0.001153\n",
       "...                           ..       ...       ...\n",
       "11      18       wife          1  0.004785  0.004390\n",
       "                 with          1  0.004785 -0.000395\n",
       "                 without       1  0.004785  0.001852\n",
       "                 words         1  0.004785  0.006125\n",
       "                 would         2  0.009569 -0.000789\n",
       "\n",
       "[287131 rows x 3 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9eaaa-896e-4360-ade0-0b20838d3155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c674bda-4ea0-4811-98c9-743646220b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76450f-8316-4e1b-84da-ce096a7ea879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d546ab0-0062-401b-923e-c3939173fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5c88ff-e4bd-4d2a-83ec-68c2d5a9b554",
   "metadata": {},
   "source": [
    "## Save To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "cdeeb431-4cca-4a9f-b77b-8e4d76a8bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vocab.to_csv('VOCAB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a5cbfe-483f-4165-bb7b-c4c4d423c230",
   "metadata": {},
   "source": [
    "# Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a277715a-c648-4b60-9035-dd54196a4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = pd.read_csv(\"TOKENS.csv\").set_index([\"book_id\", \"chap_num\", \"para_num\", \"sent_num\", \"token_num\"])\n",
    "VOCAB = pd.read_csv(\"VOCAB.csv\").set_index([\"book_id\",\"term_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "70fd2467-1c4b-47fc-8063-e7137ec9ad99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>the</th>\n",
       "      <td>3409</td>\n",
       "      <td>0.053979</td>\n",
       "      <td>4.211454</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1828</td>\n",
       "      <td>0.028945</td>\n",
       "      <td>5.110536</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1744</td>\n",
       "      <td>0.027615</td>\n",
       "      <td>5.178402</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>1553</td>\n",
       "      <td>0.024591</td>\n",
       "      <td>5.345744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1468</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>5.426950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th>ditty</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>15.874981</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entitled</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>15.874981</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deduce</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>15.874981</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rudimentary</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>15.874981</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>documentary</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>15.874981</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89979 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        n         p          i  n_chars\n",
       "book_id term_str                                       \n",
       "0       the          3409  0.053979   4.211454        3\n",
       "        to           1828  0.028945   5.110536        2\n",
       "        and          1744  0.027615   5.178402        3\n",
       "        i            1553  0.024591   5.345744        1\n",
       "        a            1468  0.023245   5.426950        1\n",
       "...                   ...       ...        ...      ...\n",
       "11      ditty           1  0.000017  15.874981        5\n",
       "        entitled        1  0.000017  15.874981        8\n",
       "        deduce          1  0.000017  15.874981        6\n",
       "        rudimentary     1  0.000017  15.874981       11\n",
       "        documentary     1  0.000017  15.874981       11\n",
       "\n",
       "[89979 rows x 4 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46003bf6-a0a3-4d6a-ad1b-dc264b02d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
